algorithm:
  module: algorithms
  name: DeepEnsembles
  params:
    num_models: 1
    warm_start_it: 6000

model:
  module: models
  name: MLP

optimizer:
  module: torch.optim
  name: Adam

dataset:
  module: data_loader
  name: SineDataset
  params:
    num_samples: 500
    domain: !!python/tuple [0, 10]

parameters:
  batch_size: 500
  num_epochs: 10000
  save_summary_steps: 1
  learning_rate: 0.01