algorithm:
  module: algorithms
  name: DeepEnsembles
  params:
    num_models: 1
    eps: 0.01
    adversarial: False
    warm_start_it: 50000

model:
  module: models
  name: MLP

optimizer:
  module: torch.optim
  name: Adam

dataset:
  module: data_loader
  name: SineDataset
  params:
    num_samples: 500
    domain: !!python/tuple [0, 10]
  # more if needed

parameters:
  batch_size: 500
  num_epochs: 10000
  save_summary_steps: 1
  learning_rate: 0.001 # ?